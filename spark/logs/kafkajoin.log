Traceback (most recent call last):
  File "/data/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/data/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o98.awaitTermination.
: org.apache.spark.sql.streaming.StreamingQueryException: Failed to construct kafka consumer
=== Streaming Query ===
Identifier: [id = f4d29673-3ea8-476c-a75f-51db60a129e5, runId = c74936fc-23bd-4352-9c00-d9cad50d7bf3]
Current Committed Offsets: {}
Current Available Offsets: {}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
Project [cast(email#48 as string) AS key#108, structstojson(named_struct(email, email#48, birthYear, birthYear#57, customer, customer#88, score, score#89), Some(Etc/UTC)) AS value#109]
+- Join Inner, (email#48 = customer#88)
   :- Project [email#48, split(birthDay#50, -)[0] AS birthYear#57]
   :  +- Project [email#48, birthDay#50]
   :     +- Filter (isnotnull(email#48) && isnotnull(birthDay#50))
   :        +- SubqueryAlias `customerrecords`
   :           +- Project [encodedCustomer#44.customerName AS customerName#47, encodedCustomer#44.email AS email#48, encodedCustomer#44.phone AS phone#49, encodedCustomer#44.birthDay AS birthDay#50]
   :              +- Project [key#28, jsontostructs(StructField(customerName,StringType,true), StructField(email,StringType,true), StructField(phone,StringType,true), StructField(birthDay,StringType,true), encodedCustomer#41, Some(Etc/UTC)) AS encodedCustomer#44]
   :                 +- Project [key#28, cast(unbase64(encodedCustomer#38) as string) AS encodedCustomer#41]
   :                    +- Project [key#28, zSetEntries#32[0].element AS encodedCustomer#38]
   :                       +- SubqueryAlias `redissortedset`
   :                          +- Project [value#25.key AS key#28, value#25.existType AS existType#29, value#25.ch AS ch#30, value#25.incr AS incr#31, value#25.zSetEntries AS zSetEntries#32]
   :                             +- Project [key#21, jsontostructs(StructField(key,StringType,true), StructField(existType,StringType,true), StructField(ch,StringType,true), StructField(incr,BooleanType,true), StructField(zSetEntries,ArrayType(StructType(StructField(element,StringType,true), StructField(score,StringType,true)),true),true), value#22, Some(Etc/UTC)) AS value#25]
   :                                +- Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
   :                                   +- StreamingExecutionRelation KafkaV2[Subscribe[redis-server]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]
   +- Project [customer#88, score#89]
      +- SubqueryAlias `customerrisk`
         +- Project [value#85.customer AS customer#88, value#85.score AS score#89, value#85.riskDate AS riskDate#90]
            +- Project [key#81, jsontostructs(StructField(customer,StringType,true), StructField(score,StringType,true), StructField(riskDate,StringType,true), value#82, Some(Etc/UTC)) AS value#85]
               +- Project [cast(key#67 as string) AS key#81, cast(value#68 as string) AS value#82]
                  +- StreamingExecutionRelation KafkaV2[Subscribe[stedi-events]], [key#67, value#68, topic#69, partition#70, offset#71L, timestamp#72, timestampType#73]

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:799)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.createConsumer(ConsumerStrategy.scala:62)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader.consumer(KafkaOffsetReader.scala:86)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$fetchEarliestOffsets$1$$anonfun$apply$8.apply(KafkaOffsetReader.scala:187)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$fetchEarliestOffsets$1$$anonfun$apply$8.apply(KafkaOffsetReader.scala:185)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$org$apache$spark$sql$kafka010$KafkaOffsetReader$$withRetriesWithoutInterrupt$1.apply$mcV$sp(KafkaOffsetReader.scala:358)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$org$apache$spark$sql$kafka010$KafkaOffsetReader$$withRetriesWithoutInterrupt$1.apply(KafkaOffsetReader.scala:357)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$org$apache$spark$sql$kafka010$KafkaOffsetReader$$withRetriesWithoutInterrupt$1.apply(KafkaOffsetReader.scala:357)
	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader.org$apache$spark$sql$kafka010$KafkaOffsetReader$$withRetriesWithoutInterrupt(KafkaOffsetReader.scala:356)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$fetchEarliestOffsets$1.apply(KafkaOffsetReader.scala:185)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader$$anonfun$fetchEarliestOffsets$1.apply(KafkaOffsetReader.scala:185)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:325)
	at org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:184)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchReader$$anonfun$getOrCreateInitialPartitionOffsets$1.apply(KafkaMicroBatchReader.scala:205)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchReader$$anonfun$getOrCreateInitialPartitionOffsets$1.apply(KafkaMicroBatchReader.scala:202)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchReader.getOrCreateInitialPartitionOffsets(KafkaMicroBatchReader.scala:202)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchReader.org$apache$spark$sql$kafka010$KafkaMicroBatchReader$$initialPartitionOffsets$lzycompute(KafkaMicroBatchReader.scala:83)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchReader.org$apache$spark$sql$kafka010$KafkaMicroBatchReader$$initialPartitionOffsets(KafkaMicroBatchReader.scala:83)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchReader.setOffsetRange(KafkaMicroBatchReader.scala:87)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1$$anonfun$5$$anonfun$apply$2.apply$mcV$sp(MicroBatchExecution.scala:353)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1$$anonfun$5$$anonfun$apply$2.apply(MicroBatchExecution.scala:353)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1$$anonfun$5$$anonfun$apply$2.apply(MicroBatchExecution.scala:353)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1$$anonfun$5.apply(MicroBatchExecution.scala:349)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1$$anonfun$5.apply(MicroBatchExecution.scala:341)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1.apply$mcZ$sp(MicroBatchExecution.scala:341)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1.apply(MicroBatchExecution.scala:337)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch$1.apply(MicroBatchExecution.scala:337)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:557)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$constructNextBatch(MicroBatchExecution.scala:337)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:183)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
	... 1 more
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:66)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:709)
	... 50 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/workspace/sparkpykafkajoin.py", line 186, in <module>
    .option("checkpointLocation", "/tmp/kafkacheckpoint2")\
  File "/data/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 103, in awaitTermination
  File "/data/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/data/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 75, in deco
pyspark.sql.utils.StreamingQueryException: 'Failed to construct kafka consumer\n=== Streaming Query ===\nIdentifier: [id = f4d29673-3ea8-476c-a75f-51db60a129e5, runId = c74936fc-23bd-4352-9c00-d9cad50d7bf3]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [cast(email#48 as string) AS key#108, structstojson(named_struct(email, email#48, birthYear, birthYear#57, customer, customer#88, score, score#89), Some(Etc/UTC)) AS value#109]\n+- Join Inner, (email#48 = customer#88)\n   :- Project [email#48, split(birthDay#50, -)[0] AS birthYear#57]\n   :  +- Project [email#48, birthDay#50]\n   :     +- Filter (isnotnull(email#48) && isnotnull(birthDay#50))\n   :        +- SubqueryAlias `customerrecords`\n   :           +- Project [encodedCustomer#44.customerName AS customerName#47, encodedCustomer#44.email AS email#48, encodedCustomer#44.phone AS phone#49, encodedCustomer#44.birthDay AS birthDay#50]\n   :              +- Project [key#28, jsontostructs(StructField(customerName,StringType,true), StructField(email,StringType,true), StructField(phone,StringType,true), StructField(birthDay,StringType,true), encodedCustomer#41, Some(Etc/UTC)) AS encodedCustomer#44]\n   :                 +- Project [key#28, cast(unbase64(encodedCustomer#38) as string) AS encodedCustomer#41]\n   :                    +- Project [key#28, zSetEntries#32[0].element AS encodedCustomer#38]\n   :                       +- SubqueryAlias `redissortedset`\n   :                          +- Project [value#25.key AS key#28, value#25.existType AS existType#29, value#25.ch AS ch#30, value#25.incr AS incr#31, value#25.zSetEntries AS zSetEntries#32]\n   :                             +- Project [key#21, jsontostructs(StructField(key,StringType,true), StructField(existType,StringType,true), StructField(ch,StringType,true), StructField(incr,BooleanType,true), StructField(zSetEntries,ArrayType(StructType(StructField(element,StringType,true), StructField(score,StringType,true)),true),true), value#22, Some(Etc/UTC)) AS value#25]\n   :                                +- Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]\n   :                                   +- StreamingExecutionRelation KafkaV2[Subscribe[redis-server]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   +- Project [customer#88, score#89]\n      +- SubqueryAlias `customerrisk`\n         +- Project [value#85.customer AS customer#88, value#85.score AS score#89, value#85.riskDate AS riskDate#90]\n            +- Project [key#81, jsontostructs(StructField(customer,StringType,true), StructField(score,StringType,true), StructField(riskDate,StringType,true), value#82, Some(Etc/UTC)) AS value#85]\n               +- Project [cast(key#67 as string) AS key#81, cast(value#68 as string) AS value#82]\n                  +- StreamingExecutionRelation KafkaV2[Subscribe[stedi-events]], [key#67, value#68, topic#69, partition#70, offset#71L, timestamp#72, timestampType#73]\n'
